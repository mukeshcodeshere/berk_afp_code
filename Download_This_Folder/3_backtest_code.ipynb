{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding data for ticker: ABUS\n",
      "Adding data for ticker: ACAD\n",
      "Adding data for ticker: ADAP\n",
      "Adding data for ticker: ADMA\n",
      "Adding data for ticker: ADPT\n",
      "Adding data for ticker: AFMD\n",
      "Adding data for ticker: AGEN\n",
      "Adding data for ticker: AGIO\n",
      "Adding data for ticker: ALDX\n",
      "Adding data for ticker: ALKS\n",
      "Adding data for ticker: ALLK\n",
      "Adding data for ticker: ALLO\n",
      "Adding data for ticker: ALNY\n",
      "Adding data for ticker: AMGN\n",
      "Adding data for ticker: AMRN\n",
      "Adding data for ticker: ANIP\n",
      "Adding data for ticker: APLS\n",
      "Adding data for ticker: ARDX\n",
      "Adding data for ticker: ARGX\n",
      "Adding data for ticker: ARVN\n",
      "Adding data for ticker: ARWR\n",
      "Adding data for ticker: ASND\n",
      "Adding data for ticker: ATRA\n",
      "Adding data for ticker: AUPH\n",
      "Adding data for ticker: AVDL\n",
      "Adding data for ticker: AVXL\n",
      "Adding data for ticker: AXSM\n",
      "Adding data for ticker: AZN\n",
      "Adding data for ticker: BCRX\n",
      "Adding data for ticker: BGNE\n",
      "Adding data for ticker: BIIB\n",
      "Adding data for ticker: BLUE\n",
      "Adding data for ticker: BMRN\n",
      "Adding data for ticker: BPMC\n",
      "Adding data for ticker: BRKR\n",
      "Adding data for ticker: CARA\n",
      "Adding data for ticker: CBAY\n",
      "Adding data for ticker: CDMO\n",
      "Adding data for ticker: CGC\n",
      "Adding data for ticker: CHRS\n",
      "Adding data for ticker: CORT\n",
      "Adding data for ticker: CPRX\n",
      "Adding data for ticker: CRMD\n",
      "Adding data for ticker: CRSP\n",
      "Adding data for ticker: CUTR\n",
      "Adding data for ticker: CYRX\n",
      "Adding data for ticker: CYTK\n",
      "Adding data for ticker: DCPH\n",
      "Adding data for ticker: DNLI\n",
      "Adding data for ticker: DVAX\n",
      "Adding data for ticker: EDIT\n",
      "Adding data for ticker: EOLS\n",
      "Adding data for ticker: ESPR\n",
      "Adding data for ticker: EXEL\n",
      "Adding data for ticker: FATE\n",
      "Adding data for ticker: FGEN\n",
      "Adding data for ticker: FOLD\n",
      "Adding data for ticker: GERN\n",
      "Adding data for ticker: GILD\n",
      "Adding data for ticker: GLPG\n",
      "Adding data for ticker: GMAB\n",
      "Adding data for ticker: GOSS\n",
      "Adding data for ticker: GTHX\n",
      "Adding data for ticker: HALO\n",
      "Adding data for ticker: HRTX\n",
      "Adding data for ticker: IBRX\n",
      "Adding data for ticker: ICLR\n",
      "Adding data for ticker: IDXX\n",
      "Adding data for ticker: IMGN\n",
      "Adding data for ticker: INCY\n",
      "Adding data for ticker: INSM\n",
      "Adding data for ticker: INVA\n",
      "Adding data for ticker: IONS\n",
      "Adding data for ticker: IOVA\n",
      "Adding data for ticker: IRWD\n",
      "Adding data for ticker: ITCI\n",
      "Adding data for ticker: JAZZ\n",
      "Adding data for ticker: KOD\n",
      "Adding data for ticker: KURA\n",
      "Adding data for ticker: LGND\n",
      "Adding data for ticker: LNTH\n",
      "Adding data for ticker: LXRX\n",
      "Adding data for ticker: MASI\n",
      "Adding data for ticker: MCRB\n",
      "Adding data for ticker: MDGL\n",
      "Adding data for ticker: MEDP\n",
      "Adding data for ticker: MGNX\n",
      "Adding data for ticker: MNKD\n",
      "Adding data for ticker: MRNA\n",
      "Adding data for ticker: MRNS\n",
      "Adding data for ticker: MRTX\n",
      "Adding data for ticker: MYGN\n",
      "Adding data for ticker: NBIX\n",
      "Adding data for ticker: NKTR\n",
      "Adding data for ticker: NSTG\n",
      "Adding data for ticker: NTLA\n",
      "Adding data for ticker: NVAX\n",
      "Adding data for ticker: OCUL\n",
      "Adding data for ticker: OGI\n",
      "Adding data for ticker: OMER\n",
      "Adding data for ticker: OPK\n",
      "Adding data for ticker: PACB\n",
      "Adding data for ticker: PBYI\n",
      "Adding data for ticker: PCRX\n",
      "Adding data for ticker: PGEN\n",
      "Adding data for ticker: PRTA\n",
      "Adding data for ticker: PTCT\n",
      "Adding data for ticker: QURE\n",
      "Adding data for ticker: RARE\n",
      "Adding data for ticker: RCKT\n",
      "Adding data for ticker: REGN\n",
      "Adding data for ticker: RGEN\n",
      "Adding data for ticker: RGNX\n",
      "Adding data for ticker: RIGL\n",
      "Adding data for ticker: RNAC\n",
      "Adding data for ticker: RVNC\n",
      "Adding data for ticker: SAGE\n",
      "Adding data for ticker: SGMO\n",
      "Adding data for ticker: SIGA\n",
      "Adding data for ticker: SNDX\n",
      "Adding data for ticker: SNY\n",
      "Adding data for ticker: SRPT\n",
      "Adding data for ticker: SUPN\n",
      "Adding data for ticker: TBPH\n",
      "Adding data for ticker: TCRT\n",
      "Adding data for ticker: TGTX\n",
      "Adding data for ticker: TLRY\n",
      "Adding data for ticker: TVTX\n",
      "Adding data for ticker: TWST\n",
      "Adding data for ticker: TXG\n",
      "Adding data for ticker: UTHR\n",
      "Adding data for ticker: VCEL\n",
      "Adding data for ticker: VERU\n",
      "Adding data for ticker: VKTX\n",
      "Adding data for ticker: VNDA\n",
      "Adding data for ticker: VYGR\n",
      "Adding data for ticker: WVE\n",
      "Adding data for ticker: XENE\n",
      "Adding data for ticker: XERS\n",
      "Adding data for ticker: ZLAB\n",
      "Adding data for ticker: ZYME\n",
      "Adding data for ticker: ZYXI\n",
      "Adding data for ticker: ARCT\n",
      "Adding data for ticker: CMRX\n",
      "Adding data for ticker: GRFS\n",
      "Adding data for ticker: RYTM\n",
      "Adding data for ticker: ALEC\n",
      "Adding data for ticker: TECH\n",
      "Adding data for ticker: BBIO\n",
      "Adding data for ticker: KRTX\n",
      "Adding data for ticker: SAVA\n",
      "Adding data for ticker: AKRO\n",
      "Adding data for ticker: BTAI\n",
      "Adding data for ticker: MRSN\n",
      "Adding data for ticker: CUE\n",
      "Adding data for ticker: INMD\n",
      "Adding data for ticker: SDGR\n",
      "Adding data for ticker: VIR\n",
      "Adding data for ticker: BNTX\n",
      "Adding data for ticker: ALT\n",
      "Adding data for ticker: CLDX\n",
      "Adding data for ticker: RPRX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 3800/14580 [00:55<02:25, 74.07it/s]"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "array assignment index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/Users/mukeshwaranbaskaran/anaconda3/envs/HW2/lib/python3.12/site-packages/joblib/externals/loky/process_executor.py\", line 463, in _process_worker\n    r = call_item()\n        ^^^^^^^^^^^\n  File \"/Users/mukeshwaranbaskaran/anaconda3/envs/HW2/lib/python3.12/site-packages/joblib/externals/loky/process_executor.py\", line 291, in __call__\n    return self.fn(*self.args, **self.kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/mukeshwaranbaskaran/anaconda3/envs/HW2/lib/python3.12/site-packages/joblib/parallel.py\", line 598, in __call__\n    return [func(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/mukeshwaranbaskaran/Documents/MFE Notes/Term 4/AFP/Code/Download_This_Folder/bt_strategy.py\", line 33, in strategy_quality\n    res = c.run()\n          ^^^^^^^\n  File \"/Users/mukeshwaranbaskaran/anaconda3/envs/HW2/lib/python3.12/site-packages/backtrader/cerebro.py\", line 1132, in run\n    runstrat = self.runstrategies(iterstrat)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/mukeshwaranbaskaran/anaconda3/envs/HW2/lib/python3.12/site-packages/backtrader/cerebro.py\", line 1298, in runstrategies\n    self._runonce(runstrats)\n  File \"/Users/mukeshwaranbaskaran/anaconda3/envs/HW2/lib/python3.12/site-packages/backtrader/cerebro.py\", line 1657, in _runonce\n    strat._once()\n  File \"/Users/mukeshwaranbaskaran/anaconda3/envs/HW2/lib/python3.12/site-packages/backtrader/lineiterator.py\", line 297, in _once\n    indicator._once()\n  File \"/Users/mukeshwaranbaskaran/anaconda3/envs/HW2/lib/python3.12/site-packages/backtrader/lineiterator.py\", line 297, in _once\n    indicator._once()\n  File \"/Users/mukeshwaranbaskaran/anaconda3/envs/HW2/lib/python3.12/site-packages/backtrader/lineiterator.py\", line 317, in _once\n    self.oncestart(self._minperiod - 1, self._minperiod)\n  File \"/Users/mukeshwaranbaskaran/anaconda3/envs/HW2/lib/python3.12/site-packages/backtrader/lineiterator.py\", line 327, in oncestart\n    self.once(start, end)\n  File \"/Users/mukeshwaranbaskaran/anaconda3/envs/HW2/lib/python3.12/site-packages/backtrader/indicators/basicops.py\", line 70, in once\n    dst[i] = func(src[i - period + 1: i + 1])\n    ~~~^^^\nIndexError: array assignment index out of range\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 184\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;66;03m# Prepare logs for each ticker's OHLC data\u001b[39;00m\n\u001b[1;32m    182\u001b[0m logs \u001b[38;5;241m=\u001b[39m {ticker: merged_equities_OHLC[merged_equities_OHLC[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTicker\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m ticker] \u001b[38;5;28;01mfor\u001b[39;00m ticker \u001b[38;5;129;01min\u001b[39;00m unique_tickers}\n\u001b[0;32m--> 184\u001b[0m results_df \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_strategies\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstrategies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# Save results to a CSV file (optional)\u001b[39;00m\n\u001b[1;32m    187\u001b[0m results_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mREPORTS_FOLDER\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/strategy_evaluation_results.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Documents/MFE Notes/Term 4/AFP/Code/Download_This_Folder/bt_strategy.py:49\u001b[0m, in \u001b[0;36mevaluate_strategies\u001b[0;34m(strategies, logs, n_trials, n_jobs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_strategies\u001b[39m(\n\u001b[1;32m     42\u001b[0m     strategies: List[Callable[[], bt\u001b[38;5;241m.\u001b[39mCerebro]],\n\u001b[1;32m     43\u001b[0m     logs: Dict[\u001b[38;5;28mstr\u001b[39m, pd\u001b[38;5;241m.\u001b[39mDataFrame],\n\u001b[1;32m     44\u001b[0m     n_trials: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m     45\u001b[0m     n_jobs: \u001b[38;5;28mint\u001b[39m\n\u001b[1;32m     46\u001b[0m ):\n\u001b[1;32m     47\u001b[0m     tasks \u001b[38;5;241m=\u001b[39m [(s, ln, l) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m strategies \u001b[38;5;28;01mfor\u001b[39;00m ln, l \u001b[38;5;129;01min\u001b[39;00m logs\u001b[38;5;241m.\u001b[39mitems()]\u001b[38;5;241m*\u001b[39mn_trials\n\u001b[0;32m---> 49\u001b[0m     stats \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstrategy_quality\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mticker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mticker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame(stats, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstrategy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mticker\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdropdown\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/HW2/lib/python3.12/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/HW2/lib/python3.12/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/HW2/lib/python3.12/site-packages/joblib/parallel.py:1754\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_retrieval():\n\u001b[1;32m   1748\u001b[0m \n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[1;32m   1751\u001b[0m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[1;32m   1752\u001b[0m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborting:\n\u001b[0;32m-> 1754\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_error_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1755\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1757\u001b[0m     \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m     \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/HW2/lib/python3.12/site-packages/joblib/parallel.py:1789\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1785\u001b[0m \u001b[38;5;66;03m# If this error job exists, immediately raise the error by\u001b[39;00m\n\u001b[1;32m   1786\u001b[0m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[1;32m   1788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1789\u001b[0m     \u001b[43merror_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/HW2/lib/python3.12/site-packages/joblib/parallel.py:745\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.get_result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    739\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel\u001b[38;5;241m.\u001b[39m_backend\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39msupports_retrieve_callback:\n\u001b[1;32m    742\u001b[0m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[1;32m    744\u001b[0m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[0;32m--> 745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_return_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/HW2/lib/python3.12/site-packages/joblib/parallel.py:763\u001b[0m, in \u001b[0;36mBatchCompletionCallBack._return_or_raise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    762\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m TASK_ERROR:\n\u001b[0;32m--> 763\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[1;32m    764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[1;32m    765\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[0;31mIndexError\u001b[0m: array assignment index out of range"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 3800/14580 [01:10<02:25, 74.07it/s]"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import logging\n",
    "import backtrader as bt\n",
    "import quantstats as qs\n",
    "import os\n",
    "from bt_strategy import (\n",
    "    evaluate_strategies,\n",
    "    buy_and_hold_strategy,\n",
    "    close_vs_sma_strategy,\n",
    "    mean_reversion_strategy,\n",
    "    prev_peak_strategy,\n",
    "    random_strategy,\n",
    "    prev_peak_nodrop_strategy,\n",
    "    close_vs_sma_nodrop_strategy,\n",
    "    mean_reversion_nodrop_strategy,\n",
    "    anti_drop_strategy\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set Pandas option to display all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Evaluate strategies with multiple trials and parallel processing\n",
    "n_trials = 10  # Number of trials per strategy per ticker\n",
    "n_jobs = -1     # Use all available CPU cores\n",
    "\n",
    "# Set up logging configuration\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Define your reporting folder (ensure this exists)\n",
    "REPORTS_FOLDER = \"QuantStats_Reports\"\n",
    "os.makedirs(REPORTS_FOLDER, exist_ok=True)  # Create the folder if it doesn't exist\n",
    "\n",
    "# Constants\n",
    "DB_PATH = r\"/Users/mukeshwaranbaskaran/Documents/MFE Notes/Term 4/AFP/Code/Download_This_Folder/1_financial_data.db\"\n",
    "START_DATE = '2020-01-01'\n",
    "END_DATE = '2023-12-31'\n",
    "\n",
    "# Function to load distinct tickers from the SQLite database\n",
    "def load_distinct_tickers_from_db():\n",
    "    \"\"\"Load distinct tickers from the database within the specified date range.\"\"\"\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    query = \"\"\"\n",
    "        SELECT DISTINCT ticker\n",
    "        FROM merged_data\n",
    "        WHERE date BETWEEN ? AND ?\n",
    "    \"\"\"\n",
    "    distinct_tickers = pd.read_sql(query, conn, params=(START_DATE, END_DATE))\n",
    "    conn.close()\n",
    "    return distinct_tickers\n",
    "\n",
    "# Load tickers and order alphabetically\n",
    "tickers = load_distinct_tickers_from_db()\n",
    "\n",
    "# Assuming the result is a DataFrame, extract the 'ticker' column\n",
    "tickers_list_full = tickers['ticker'].tolist()\n",
    "\n",
    "tickers_list_mini = ['REGN','UTHR','CLDX','XOMA']\n",
    "#tickers_list_mini = ['REGN','UTHR','XOMA']#,'XOMA']\n",
    "# Function to load data from the SQLite database\n",
    "def load_data_from_db(tickers_list):\n",
    "    \"\"\"Load data from the database within the specified date range, excluding tickers with no data.\"\"\"\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    \n",
    "    # Convert tickers_list into a format suitable for the SQL IN clause (comma-separated string)\n",
    "    tickers_tuple = tuple(tickers_list)\n",
    "    \n",
    "    # Make sure tickers_tuple is not empty to prevent SQL errors\n",
    "    if not tickers_tuple:\n",
    "        return pd.DataFrame()  # Return an empty DataFrame if no tickers are provided\n",
    "\n",
    "    query = \"\"\"\n",
    "        SELECT *\n",
    "        FROM merged_data\n",
    "        WHERE date BETWEEN ? AND ?\n",
    "        AND ticker IN ({})\n",
    "    \"\"\".format(','.join(['?'] * len(tickers_tuple)))  # Dynamically insert placeholders for each ticker\n",
    "\n",
    "    # Run the query with the tickers_list as parameters\n",
    "    params = (START_DATE, END_DATE) + tickers_tuple\n",
    "    merged_data = pd.read_sql(query, conn, params=params)\n",
    "    \n",
    "    conn.close()\n",
    "    return merged_data\n",
    "\n",
    "# Load and preprocess data\n",
    "merged_data = load_data_from_db(tickers_list_full)\n",
    "merged_data['date'] = pd.to_datetime(merged_data['date'], errors='coerce')\n",
    "merged_data.sort_values('date', inplace=True)\n",
    "merged_data.set_index('date', inplace=True)\n",
    "\n",
    "equities_cols_needed = ['permno', 'cusip', 'bid', 'ask', 'vol', 'shrout', 'prc', 'mktcap', 'ticker']\n",
    "merged_data_equities = merged_data[equities_cols_needed]\n",
    "\n",
    "# Drop duplicate rows & reset index\n",
    "merged_data_equities_no_duplicates = merged_data_equities.drop_duplicates()\n",
    "merged_data_equities_no_duplicates_reset_index = merged_data_equities_no_duplicates.reset_index()\n",
    "\n",
    "# ERRORS WITH LESS THAN 800 rows of data per ticker\n",
    "def filter_tickers_with_min_rows(df, min_rows=800):\n",
    "    # Group by 'ticker' and count the rows for each ticker\n",
    "    ticker_counts = df.groupby('ticker').size()\n",
    "    \n",
    "    # Identify tickers with more than the minimum required rows\n",
    "    valid_tickers = ticker_counts[ticker_counts >= min_rows].index.tolist()\n",
    "    \n",
    "    # Filter the DataFrame to only include rows for these tickers\n",
    "    filtered_data_800 = df[df['ticker'].isin(valid_tickers)]\n",
    "    \n",
    "    #print(f\"Filtered DataFrame with only tickers that have at least {min_rows} rows.\")\n",
    "    \n",
    "    return filtered_data_800\n",
    "\n",
    "filtered_data_800 = filter_tickers_with_min_rows(merged_data_equities_no_duplicates_reset_index)\n",
    "\n",
    "# Convert to OHLC format for backtrader\n",
    "def convert_to_ohlc_format(df):\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    grouped = df.groupby(['date', 'ticker'])\n",
    "    result = grouped.agg(\n",
    "        Open=('bid', 'first'),\n",
    "        High=('ask', 'max'),\n",
    "        Low=('bid', 'min'),\n",
    "        Close=('prc', 'last'),  # Use 'prc' for closing price here\n",
    "        Adj_Close=('prc', 'last'),\n",
    "        Volume=('vol', 'sum')\n",
    "    ).reset_index()\n",
    "    result.set_index('date', inplace=True)\n",
    "    result.rename(columns={'ticker': 'Ticker'}, inplace=True)\n",
    "    return result\n",
    "\n",
    "merged_equities_OHLC = convert_to_ohlc_format(filtered_data_800)\n",
    "\n",
    "# BACKTESTING \n",
    "\n",
    "# Initialize Cerebro engine\n",
    "cerebro = bt.Cerebro()\n",
    "\n",
    "# Add data feeds for all tickers\n",
    "unique_tickers = merged_equities_OHLC['Ticker'].unique()\n",
    "\n",
    "for ticker in unique_tickers:\n",
    "    print(f\"Adding data for ticker: {ticker}\")\n",
    "    \n",
    "    # Filter data for the current ticker\n",
    "    ticker_data = merged_equities_OHLC[merged_equities_OHLC['Ticker'] == ticker]\n",
    "    ticker_data.index = pd.to_datetime(ticker_data.index)\n",
    "    ticker_data = ticker_data[['Open', 'High', 'Low', 'Close', 'Volume']]\n",
    "    \n",
    "    # Define custom PandasData feed for backtrader\n",
    "    class PandasData(bt.feeds.PandasData):\n",
    "        params = (\n",
    "            ('datetime', None),\n",
    "            ('open', 'Open'),\n",
    "            ('high', 'High'),\n",
    "            ('low', 'Low'),\n",
    "            ('close', 'Close'),\n",
    "            ('volume', 'Volume'),\n",
    "            ('openinterest', None),\n",
    "        )\n",
    "\n",
    "    # Add this ticker's data feed to Cerebro\n",
    "    data_feed = PandasData(dataname=ticker_data)\n",
    "    cerebro.adddata(data_feed, name=ticker)\n",
    "\n",
    "# Define strategies to evaluate\n",
    "strategies = [\n",
    "    buy_and_hold_strategy,\n",
    "    close_vs_sma_strategy,\n",
    "    mean_reversion_strategy,\n",
    "    prev_peak_strategy,\n",
    "    random_strategy,\n",
    "    prev_peak_nodrop_strategy,\n",
    "    close_vs_sma_nodrop_strategy,\n",
    "    mean_reversion_nodrop_strategy,\n",
    "    anti_drop_strategy\n",
    "]\n",
    "\n",
    "# Prepare logs for each ticker's OHLC data\n",
    "logs = {ticker: merged_equities_OHLC[merged_equities_OHLC['Ticker'] == ticker] for ticker in unique_tickers}\n",
    "\n",
    "results_df = evaluate_strategies(strategies, logs, n_trials, n_jobs)\n",
    "\n",
    "# Save results to a CSV file (optional)\n",
    "results_df.to_csv(f\"{REPORTS_FOLDER}/strategy_evaluation_results.csv\", index=False)\n",
    "\n",
    "# Function to generate QuantStats reports\n",
    "def generate_quantstats_report(logs, strategies):\n",
    "    \"\"\"Generate and save one QuantStats report per strategy.\"\"\"\n",
    "    if not logs:\n",
    "        logging.warning(\"No logs available to generate reports.\")\n",
    "        return\n",
    "\n",
    "    for strategy in strategies:\n",
    "        try:\n",
    "            combined_data = pd.DataFrame()  # To store combined data for each strategy\n",
    "            for ticker in logs.keys():  # Use only the tickers in 'logs'\n",
    "                ticker_data = logs[ticker]\n",
    "                \n",
    "                # Ensure the ticker data is in the format QuantStats expects\n",
    "                # 'Close' as the column representing the stock's price (closing price)\n",
    "                combined_data[ticker] = ticker_data['Close']\n",
    "\n",
    "            # Create a QuantStats report for the combined data of this strategy\n",
    "            qs.extend_pandas()  # Ensure QuantStats can extend Pandas functionalities\n",
    "            report_filename = f'quantstats_report_{strategy.__name__}.html'\n",
    "            report_path = os.path.join(REPORTS_FOLDER, report_filename)\n",
    "            qs.reports.html(combined_data, output=report_path, title=f'QuantStats Report for {strategy.__name__}')\n",
    "            logging.info(f\"QuantStats report generated for strategy: {strategy.__name__}\")\n",
    "            logging.info(f\"Report saved to: {report_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error generating QuantStats report for strategy {strategy.__name__}: {e}\")\n",
    "\n",
    "# Generate the QuantStats reports for each strategy\n",
    "generate_quantstats_report(logs, strategies)\n",
    "\n",
    "# EVALUATION - Strategy performance stats\n",
    "# Grouping results by strategy name to calculate mean value and max drawdown\n",
    "strategy_summary = results_df.groupby('strategy').agg(\n",
    "    mean_value=('value', 'mean'),\n",
    "    max_drawdown=('dropdown', 'max')\n",
    ")\n",
    "\n",
    "# Print the evaluation summary\n",
    "logging.info(\"Strategy Evaluation Summary:\")\n",
    "logging.info(strategy_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HW2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
